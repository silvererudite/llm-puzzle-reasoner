# llm-puzzle-reasoner

Large Language Models (LLMs) often struggle with compositional reasoning tasks like the 8 queens puzzle due to their autoregressive nature. While trained on vast amounts of data, these models typically rely on pattern matching and probabilistic inference rather than explicit, step-by-step logical reasoning. This means they frequently fail at problems requiring systematic decomposition, constraint satisfaction, and strategic search through solution spaces. The challenge highlights a critical limitation of current AI systems: their difficulty in performing deliberate, modular reasoning that breaks complex problems into interpretable sub-components and reconstructs solutions through principled combination. Developing models that can genuinely decompose and solve problems compositionally could potentially bring us closer to more robust, generalizable, and trustworthy intelligent systems.
